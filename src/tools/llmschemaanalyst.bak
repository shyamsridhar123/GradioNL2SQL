"""
LLM-Powered Schema Analyst Tool
Uses GPT-4 to semantically understand queries and map them to database schema
"""

import os
import sys
from typing import Dict, Any, List
import json
from openai import AzureOpenAI

# Add parent directory to path to import database modules
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from database.connection import DatabaseConnection
from database.config import DatabaseConfig
from database.schema_inspector import SchemaInspector
from utils.logging_config import get_logger

logger = get_logger("text2sql.llm_schema_analyst")

class LLMSchemaAnalystTool:
    """
    LLM-powered schema analysis tool that uses Azure OpenAI to understand
    user queries and identify the most relevant database tables and columns.
    """
    
    def __init__(self):
        # Initialize database connection
        self.config = DatabaseConfig.from_env(use_managed_identity=False)
        self.db_connection = DatabaseConnection(self.config)
        self.schema_inspector = SchemaInspector(self.db_connection)
        
        # Initialize Azure OpenAI client
        self.client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        
        # Cache schema to avoid repeated DB calls
        self._schema_cache = None
        self._load_schema_cache()
        
    def _load_schema_cache(self):
        """Load and cache the complete schema once"""
        try:
            self._schema_cache = self.schema_inspector.get_all_tables()
            logger.info(f"✅ LLM Schema cache loaded: {len(self._schema_cache)} tables")
        except Exception as e:
            logger.error(f"❌ Failed to load LLM schema cache: {e}")
            self._schema_cache = []
    
    def analyze_schema(self, user_query: str) -> str:
        """
        Analyze user query and return relevant schema information using LLM semantic understanding
        """
        try:
            # Get all tables from cache
            all_tables = self._schema_cache if self._schema_cache else []
            
            if not all_tables:
                return "No tables found in database schema"
            
            # Create a condensed schema summary for the LLM
            schema_summary = "Database Schema:\n"
            for table in all_tables:
                schema_summary += f"\n{table.schema}.{table.name}:\n"
                for col in table.columns[:5]:  # First 5 columns
                    pk = " [PK]" if col.is_primary_key else ""
                    fk = " [FK]" if col.is_foreign_key else ""
                    schema_summary += f"  - {col.name}: {col.data_type}{pk}{fk}\n"
                if len(table.columns) > 5:
                    schema_summary += f"  ... and {len(table.columns) - 5} more columns\n"
            
            # Prepare LLM prompt for schema analysis
            system_prompt = """You are a database schema expert. Given a user query and database schema, identify the most relevant tables and columns needed to answer the query.

Return a JSON response with this structure:
{
    "relevant_tables": ["table1", "table2"],
    "reasoning": "Why these tables are relevant",
    "confidence": 0.9
}

Focus on:
1. Tables that contain data needed for the query
2. Tables that need to be joined to get complete results
3. Columns that match query concepts (customer, sales, region, etc.)

Be smart about semantic matching - "sales" could map to "SalesOrderHeader", "customer" to "Customer", etc."""

            user_prompt = f"""
User Query: {user_query}

Available Schema:
{schema_summary}

Analyze this query and identify the most relevant tables needed to answer it.
"""

            # Call GPT-4 for intelligent analysis
            response = self.client.chat.completions.create(
                model="gpt-4o",  # Use GPT-4 for better reasoning
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            # Parse LLM response
            llm_result = response.choices[0].message.content.strip()
            
            try:
                # Try to parse as JSON
                analysis = json.loads(llm_result)
                relevant_table_names = analysis.get("relevant_tables", [])
                reasoning = analysis.get("reasoning", "")
                confidence = analysis.get("confidence", 0.5)
                
            except json.JSONDecodeError:
                # Fallback: extract table names from text response
                logger.warning("Failed to parse LLM JSON response, using text fallback")
                relevant_table_names = []
                for table in all_tables:
                    if table.name.lower() in llm_result.lower():
                        relevant_table_names.append(table.name)
                reasoning = llm_result
                confidence = 0.5
            
            # Find the actual table objects
            relevant_tables = []
            for table in all_tables:
                if any(name.lower() in table.name.lower() for name in relevant_table_names):
                    relevant_tables.append(table)
            
            # If no tables found via LLM, fall back to keyword matching
            if not relevant_tables:
                logger.warning("LLM found no relevant tables, falling back to keyword matching")
                query_lower = user_query.lower()
                for table in all_tables:
                    if any(word in table.name.lower() for word in query_lower.split() if len(word) > 3):
                        relevant_tables.append(table)
            
            # Format the result
            if not relevant_tables:
                return "No relevant tables found for this query"
            
            # Format detailed schema for the relevant tables
            result = f"RELEVANT SCHEMA (Confidence: {confidence:.1f}):\n"
            if reasoning:
                result += f"Analysis: {reasoning}\n\n"
            
            for table in relevant_tables[:3]:  # Limit to top 3 tables
                result += f"Table: {table.schema}.{table.name}\n"
                for col in table.columns:
                    pk = " [PK]" if col.is_primary_key else ""
                    fk = " [FK]" if col.is_foreign_key else ""
                    nullable = "NULL" if col.is_nullable else "NOT NULL"
                    result += f"  - {col.name}: {col.data_type} {nullable}{pk}{fk}\n"
                result += "\n"
            
            return result
            
        except Exception as e:
            logger.error(f"Error in LLM schema analysis: {e}")
            return f"Error analyzing schema: {str(e)}"
